{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de8691a-797d-45f0-b5dd-7023a481d9fe",
   "metadata": {},
   "source": [
    "cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "942d5b31-3354-4735-b95e-68fa10f2c682",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 27 from gen9doublesubers-2374223819.log\n",
      "Parsed 11 from gen9doublesubers-2374225913.log\n",
      "Parsed 25 from gen9doublesubers-2374228798.log\n",
      "Parsed 29 from gen9doublesubers-2374524555.log\n",
      "Parsed 19 from gen9doublesubers-2374529916.log\n",
      "Parsed 22 from gen9doublesubers-2374533533.log\n",
      "Parsed 21 from gen9doublesubers-2374545013.log\n",
      "Parsed 15 from gen9doublesubers-2374589924.log\n",
      "Parsed 6 from gen9doublesubers-2374592302.log\n",
      "Parsed 35 from gen9doublesubers-2374595064.log\n",
      "Parsed 16 from gen9doublesubers-2374600429.log\n",
      "Parsed 20 from gen9doublesubers-2374603218-i2jq8es1gtleug5vxnmvfn6xrqy58c1pw.log\n",
      "Parsed 25 from gen9doublesubers-2374608156-cf6wq2nh5is68mk15iq4j1vx072jnkgpw.log\n",
      "Parsed 41 from gen9doublesubers-2374618018.log\n",
      "Parsed 22 from gen9doublesubers-2374624385.log\n",
      "Parsed 28 from gen9doublesubers-2374633057-ky2fuunboavtsa55fwspvn676r1p2hipw.log\n",
      "\n",
      "Total state-action pairs collected: 362\n",
      "Example sample: ('opponent switch Maushold-Four opponent switch Archaludon', 'switch Amoonguss')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def parse_battle_log(log_text, username):\n",
    "    lines = [ln.strip() for ln in log_text.splitlines() if ln.strip()]\n",
    "    player_role = None\n",
    "    \n",
    "    for ln in lines:\n",
    "        if ln.startswith(\"|player|\"):\n",
    "            parts = ln.split(\"|\")\n",
    "            if len(parts) >= 4:\n",
    "                role = parts[2]  # 'p1' or 'p2'\n",
    "                name = parts[3]\n",
    "                if name.lower() == username.lower():\n",
    "                    player_role = role\n",
    "                    break\n",
    "    if player_role is None:\n",
    "        raise ValueError(\"Username not found in log\")\n",
    "\n",
    "    data = []\n",
    "    context_lines = []\n",
    "    current_turn = 0\n",
    "\n",
    "    for ln in lines:\n",
    "        if ln.startswith(\"|j|\") or ln.startswith(\"|l|\") or ln.startswith(\"|c|\"):\n",
    "            continue\n",
    "        if ln.startswith(\"|turn|\"):\n",
    "            try:\n",
    "                current_turn = int(ln.split(\"|\")[2])\n",
    "            except:\n",
    "                current_turn += 1\n",
    "            context_lines.append(f\"Turn {current_turn}\")\n",
    "            continue\n",
    "        if ln.startswith(\"|move|\") or ln.startswith(\"|switch|\"):\n",
    "            parts = ln.split(\"|\")\n",
    "            action_type = parts[1]\n",
    "            actor = parts[2]\n",
    "            if actor.startswith(player_role):\n",
    "                if action_type == \"move\":\n",
    "                    move_name = parts[3]\n",
    "                    action_text = f\"move {move_name}\"\n",
    "                elif action_type == \"switch\":\n",
    "                    poke_name = parts[3].split(',')[0]\n",
    "                    action_text = f\"switch {poke_name}\"\n",
    "                state_text = \" \".join(context_lines)\n",
    "                data.append((state_text, action_text))\n",
    "                context_lines.append(action_text)\n",
    "            else:\n",
    "                if action_type == \"move\":\n",
    "                    move_name = parts[3]\n",
    "                    context_lines.append(f\"opponent move {move_name}\")\n",
    "                elif action_type == \"switch\":\n",
    "                    opp_poke = parts[3].split(',')[0]\n",
    "                    context_lines.append(f\"opponent switch {opp_poke}\")\n",
    "        elif ln.startswith(\"|faint|\"):\n",
    "            faint_parts = ln.split(\"|\")\n",
    "            if len(faint_parts) >= 3:\n",
    "                faint_actor = faint_parts[2]\n",
    "                faint_text = \"ally fainted\" if faint_actor.startswith(player_role) else \"opponent fainted\"\n",
    "                context_lines.append(faint_text)\n",
    "\n",
    "    return data\n",
    "\n",
    "# === ITERATE THROUGH FOLDER ===\n",
    "\n",
    "username = \"coatoverwatch\"\n",
    "log_folder = \"DoubleBattleLogs\"\n",
    "all_samples = []\n",
    "\n",
    "for filename in os.listdir(log_folder):\n",
    "    if filename.endswith(\".log\"):\n",
    "        filepath = os.path.join(log_folder, filename)\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                log_text = f.read()\n",
    "                samples = parse_battle_log(log_text, username)\n",
    "                all_samples.extend(samples)\n",
    "                print(f\"Parsed {len(samples)} from {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {filename}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal state-action pairs collected: {len(all_samples)}\")\n",
    "print(\"Example sample:\", all_samples[0] if all_samples else \"No samples found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d46e8fa-a4e7-473a-ba27-d9ef04b3f531",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dcffa4-a4bc-48c8-9d5d-38e4a8af46e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ac86dcb-b880-45b1-9f37-de6177b0148d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "# Suppose states and actions are collected in lists\n",
    "states = [s for (s, a) in all_samples]\n",
    "actions = [a for (s, a) in all_samples]\n",
    "\n",
    "# Build a tokenizer on the states\n",
    "tokenizer = Tokenizer(num_words=None, filters=\"\", lower=False, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(states)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding or OOV\n",
    "# Convert state texts to sequences of integers\n",
    "state_sequences = tokenizer.texts_to_sequences(states)\n",
    "# Pad sequences to a fixed length (maxlen)\n",
    "maxlen = 200  # you can choose a max length, e.g. 200 tokens\n",
    "state_sequences = pad_sequences(state_sequences, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "# Create mapping for actions to integer labels\n",
    "action_to_idx = {act: i for i, act in enumerate(sorted(set(actions)))}\n",
    "idx_to_action = {i: act for act, i in action_to_idx.items()}\n",
    "num_actions = len(action_to_idx)\n",
    "# Convert actions to numeric labels\n",
    "action_labels = [action_to_idx[a] for a in actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e1d3d-5ceb-4ab3-b972-6480bd9fe176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4654101-4bf6-47c3-9f5f-22209f29e907",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 200, 64)           28160     \n",
      " ng_4 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_block_4 (Trans  (None, 200, 64)           83200     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " global_average_pooling1d_4  (None, 64)                0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 28)                1820      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117340 (458.36 KB)\n",
      "Trainable params: 117340 (458.36 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define model hyperparameters\n",
    "embed_dim = 64   # Embedding dimension for tokens\n",
    "num_heads = 4    # Number of attention heads\n",
    "ff_dim = 128     # Hidden layer size in transformer feed-forward network\n",
    "maxlen = 200     # Sequence length (should match the padding length used)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # total vocabulary size\n",
    "num_actions = len(action_to_idx)  # number of output classes\n",
    "\n",
    "# 1. Token and Position Embedding Layer\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb   = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions  # add token embedding and position embedding\n",
    "\n",
    "# 2. Transformer Encoder Block\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "    def call(self, inputs, training=False):\n",
    "        # Self-attention\n",
    "        attn_output = self.att(inputs, inputs)           # self-attend\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)     # add & norm\n",
    "        # Feed-forward\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)        # add & norm\n",
    "        return out2\n",
    "\n",
    "# 3. Build the model using the layers above\n",
    "inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "# Add one or more Transformer blocks\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "# (Optionally, you could stack multiple TransformerBlock layers for a deeper model)\n",
    "# Pool the sequence to get a fixed-size vector\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)                        # a bit of dropout for regularization\n",
    "x = layers.Dense(64, activation='relu')(x)        # a small dense layer to mix features\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(num_actions, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c13ac2e6-f000-46ef-a782-d984bb7241a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 2.3214 - accuracy: 0.2561 - val_loss: 2.7629 - val_accuracy: 0.1918\n",
      "Epoch 2/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 2.2811 - accuracy: 0.2457 - val_loss: 2.8554 - val_accuracy: 0.2055\n",
      "Epoch 3/200\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 2.4244 - accuracy: 0.2249 - val_loss: 2.7493 - val_accuracy: 0.1370\n",
      "Epoch 4/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 2.2595 - accuracy: 0.2699 - val_loss: 2.8206 - val_accuracy: 0.1507\n",
      "Epoch 5/200\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 2.3249 - accuracy: 0.2457 - val_loss: 2.7933 - val_accuracy: 0.2329\n",
      "Epoch 6/200\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 2.2484 - accuracy: 0.2595 - val_loss: 2.7214 - val_accuracy: 0.1918\n",
      "Epoch 7/200\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 2.2288 - accuracy: 0.2664 - val_loss: 2.7754 - val_accuracy: 0.2055\n",
      "Epoch 8/200\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 2.3031 - accuracy: 0.2491 - val_loss: 2.8103 - val_accuracy: 0.2055\n",
      "Epoch 9/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 2.2775 - accuracy: 0.2595 - val_loss: 2.7989 - val_accuracy: 0.1370\n",
      "Epoch 10/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 2.2049 - accuracy: 0.2768 - val_loss: 2.7978 - val_accuracy: 0.1918\n",
      "Epoch 11/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 2.2335 - accuracy: 0.2907 - val_loss: 2.8690 - val_accuracy: 0.1233\n",
      "Epoch 12/200\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 2.2195 - accuracy: 0.2664 - val_loss: 2.7887 - val_accuracy: 0.1644\n",
      "Epoch 13/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 2.1718 - accuracy: 0.2768 - val_loss: 2.7437 - val_accuracy: 0.2192\n",
      "Epoch 14/200\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 2.2205 - accuracy: 0.2457 - val_loss: 2.9322 - val_accuracy: 0.1781\n",
      "Epoch 15/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 2.1786 - accuracy: 0.2907 - val_loss: 2.9041 - val_accuracy: 0.1507\n",
      "Epoch 16/200\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 2.1366 - accuracy: 0.2595 - val_loss: 2.9072 - val_accuracy: 0.1507\n",
      "Epoch 17/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 2.1135 - accuracy: 0.3045 - val_loss: 2.8900 - val_accuracy: 0.2329\n",
      "Epoch 18/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 2.0925 - accuracy: 0.3149 - val_loss: 2.8576 - val_accuracy: 0.1781\n",
      "Epoch 19/200\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 2.1584 - accuracy: 0.2422 - val_loss: 2.9242 - val_accuracy: 0.1644\n",
      "Epoch 20/200\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 2.1081 - accuracy: 0.2872 - val_loss: 2.8637 - val_accuracy: 0.1644\n",
      "Epoch 21/200\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 2.0927 - accuracy: 0.2837 - val_loss: 2.9423 - val_accuracy: 0.1781\n",
      "Epoch 22/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 2.0490 - accuracy: 0.3045 - val_loss: 2.9905 - val_accuracy: 0.1918\n",
      "Epoch 23/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 2.1405 - accuracy: 0.2595 - val_loss: 2.9635 - val_accuracy: 0.1507\n",
      "Epoch 24/200\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 2.0542 - accuracy: 0.3045 - val_loss: 2.9351 - val_accuracy: 0.1507\n",
      "Epoch 25/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 2.1232 - accuracy: 0.2699 - val_loss: 3.1831 - val_accuracy: 0.1507\n",
      "Epoch 26/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 2.2678 - accuracy: 0.2768 - val_loss: 3.0499 - val_accuracy: 0.1918\n",
      "Epoch 27/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 2.1969 - accuracy: 0.2803 - val_loss: 3.0081 - val_accuracy: 0.2055\n",
      "Epoch 28/200\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 2.1045 - accuracy: 0.2803 - val_loss: 2.9645 - val_accuracy: 0.1507\n",
      "Epoch 29/200\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 2.1287 - accuracy: 0.2768 - val_loss: 2.9874 - val_accuracy: 0.1507\n",
      "Epoch 30/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 2.0723 - accuracy: 0.3114 - val_loss: 3.0605 - val_accuracy: 0.0685\n",
      "Epoch 31/200\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 2.0776 - accuracy: 0.2872 - val_loss: 3.0354 - val_accuracy: 0.1507\n",
      "Epoch 32/200\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 2.0098 - accuracy: 0.3253 - val_loss: 2.9028 - val_accuracy: 0.1644\n",
      "Epoch 33/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 2.0828 - accuracy: 0.2941 - val_loss: 2.9373 - val_accuracy: 0.1233\n",
      "Epoch 34/200\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 2.1068 - accuracy: 0.2837 - val_loss: 2.9510 - val_accuracy: 0.1781\n",
      "Epoch 35/200\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 2.0289 - accuracy: 0.3114 - val_loss: 3.0323 - val_accuracy: 0.1370\n",
      "Epoch 36/200\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.9778 - accuracy: 0.3045 - val_loss: 2.9950 - val_accuracy: 0.1370\n",
      "Epoch 37/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 2.0067 - accuracy: 0.3080 - val_loss: 2.9785 - val_accuracy: 0.1370\n",
      "Epoch 38/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.9337 - accuracy: 0.3495 - val_loss: 2.9959 - val_accuracy: 0.1507\n",
      "Epoch 39/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 2.0100 - accuracy: 0.3010 - val_loss: 3.0706 - val_accuracy: 0.1507\n",
      "Epoch 40/200\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.8977 - accuracy: 0.3218 - val_loss: 3.0446 - val_accuracy: 0.1918\n",
      "Epoch 41/200\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.8914 - accuracy: 0.3564 - val_loss: 3.0218 - val_accuracy: 0.1781\n",
      "Epoch 42/200\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.9232 - accuracy: 0.3287 - val_loss: 2.9922 - val_accuracy: 0.1644\n",
      "Epoch 43/200\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.8610 - accuracy: 0.3322 - val_loss: 3.0481 - val_accuracy: 0.1644\n",
      "Epoch 44/200\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.8027 - accuracy: 0.3910 - val_loss: 3.0582 - val_accuracy: 0.2192\n",
      "Epoch 45/200\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.8135 - accuracy: 0.3529 - val_loss: 3.1465 - val_accuracy: 0.1918\n",
      "Epoch 46/200\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.9328 - accuracy: 0.3322 - val_loss: 3.1671 - val_accuracy: 0.1644\n",
      "Epoch 47/200\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.8659 - accuracy: 0.3737 - val_loss: 3.1652 - val_accuracy: 0.1781\n",
      "Epoch 48/200\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.8300 - accuracy: 0.3564 - val_loss: 3.1840 - val_accuracy: 0.2192\n",
      "Epoch 49/200\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1.8144 - accuracy: 0.4118 - val_loss: 3.2434 - val_accuracy: 0.1644\n",
      "Epoch 50/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.7866 - accuracy: 0.3737 - val_loss: 3.1710 - val_accuracy: 0.1233\n",
      "Epoch 51/200\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.7487 - accuracy: 0.4221 - val_loss: 3.1687 - val_accuracy: 0.1781\n",
      "Epoch 52/200\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.7975 - accuracy: 0.4048 - val_loss: 3.1357 - val_accuracy: 0.1918\n",
      "Epoch 53/200\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.7612 - accuracy: 0.3910 - val_loss: 3.1242 - val_accuracy: 0.1781\n",
      "Epoch 54/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.7440 - accuracy: 0.4014 - val_loss: 3.1096 - val_accuracy: 0.1370\n",
      "Epoch 55/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.7155 - accuracy: 0.4325 - val_loss: 3.2305 - val_accuracy: 0.1918\n",
      "Epoch 56/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.6735 - accuracy: 0.4429 - val_loss: 3.2831 - val_accuracy: 0.2055\n",
      "Epoch 57/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.7346 - accuracy: 0.3806 - val_loss: 3.3350 - val_accuracy: 0.2055\n",
      "Epoch 58/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.7579 - accuracy: 0.4014 - val_loss: 3.3988 - val_accuracy: 0.2055\n",
      "Epoch 59/200\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 1.6656 - accuracy: 0.4048 - val_loss: 3.3361 - val_accuracy: 0.2055\n",
      "Epoch 60/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.6157 - accuracy: 0.4429 - val_loss: 3.3905 - val_accuracy: 0.2055\n",
      "Epoch 61/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.6180 - accuracy: 0.4221 - val_loss: 3.3282 - val_accuracy: 0.1918\n",
      "Epoch 62/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.5803 - accuracy: 0.4394 - val_loss: 3.4150 - val_accuracy: 0.1644\n",
      "Epoch 63/200\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.5759 - accuracy: 0.4533 - val_loss: 3.4083 - val_accuracy: 0.1918\n",
      "Epoch 64/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.6151 - accuracy: 0.4464 - val_loss: 3.4124 - val_accuracy: 0.1781\n",
      "Epoch 65/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.4808 - accuracy: 0.4810 - val_loss: 3.4793 - val_accuracy: 0.1781\n",
      "Epoch 66/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.7715 - accuracy: 0.4048 - val_loss: 3.5595 - val_accuracy: 0.2329\n",
      "Epoch 67/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.5969 - accuracy: 0.4325 - val_loss: 3.5940 - val_accuracy: 0.1781\n",
      "Epoch 68/200\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.5848 - accuracy: 0.4394 - val_loss: 3.6776 - val_accuracy: 0.2055\n",
      "Epoch 69/200\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.6873 - accuracy: 0.4533 - val_loss: 3.5349 - val_accuracy: 0.1918\n",
      "Epoch 70/200\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.6337 - accuracy: 0.4256 - val_loss: 3.5171 - val_accuracy: 0.1918\n",
      "Epoch 71/200\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.5188 - accuracy: 0.4671 - val_loss: 3.5408 - val_accuracy: 0.2055\n",
      "Epoch 72/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.5215 - accuracy: 0.4706 - val_loss: 3.5819 - val_accuracy: 0.2055\n",
      "Epoch 73/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.5740 - accuracy: 0.4533 - val_loss: 3.6072 - val_accuracy: 0.1918\n",
      "Epoch 74/200\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.4907 - accuracy: 0.4948 - val_loss: 3.6549 - val_accuracy: 0.1918\n",
      "Epoch 75/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.4082 - accuracy: 0.4913 - val_loss: 3.7084 - val_accuracy: 0.1918\n",
      "Epoch 76/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.4124 - accuracy: 0.5052 - val_loss: 3.7286 - val_accuracy: 0.1781\n",
      "Epoch 77/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.4115 - accuracy: 0.5087 - val_loss: 3.6961 - val_accuracy: 0.1918\n",
      "Epoch 78/200\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.4145 - accuracy: 0.4844 - val_loss: 3.7615 - val_accuracy: 0.2055\n",
      "Epoch 79/200\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.3374 - accuracy: 0.5190 - val_loss: 3.8427 - val_accuracy: 0.1781\n",
      "Epoch 80/200\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.3482 - accuracy: 0.5260 - val_loss: 3.8381 - val_accuracy: 0.1918\n",
      "Epoch 81/200\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.4468 - accuracy: 0.4844 - val_loss: 4.0386 - val_accuracy: 0.2329\n",
      "Epoch 82/200\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.4689 - accuracy: 0.5017 - val_loss: 3.8580 - val_accuracy: 0.2055\n",
      "Epoch 83/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.4149 - accuracy: 0.4740 - val_loss: 3.9156 - val_accuracy: 0.2329\n",
      "Epoch 84/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.3855 - accuracy: 0.5052 - val_loss: 4.0664 - val_accuracy: 0.1507\n",
      "Epoch 85/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.4624 - accuracy: 0.4810 - val_loss: 4.1636 - val_accuracy: 0.1918\n",
      "Epoch 86/200\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.4611 - accuracy: 0.4740 - val_loss: 3.9311 - val_accuracy: 0.1644\n",
      "Epoch 87/200\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.5174 - accuracy: 0.4844 - val_loss: 3.9077 - val_accuracy: 0.2192\n",
      "Epoch 88/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.4547 - accuracy: 0.5017 - val_loss: 4.1225 - val_accuracy: 0.2055\n",
      "Epoch 89/200\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.3478 - accuracy: 0.5087 - val_loss: 4.0807 - val_accuracy: 0.1781\n",
      "Epoch 90/200\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 1.2996 - accuracy: 0.5190 - val_loss: 4.1268 - val_accuracy: 0.1781\n",
      "Epoch 91/200\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.3059 - accuracy: 0.5398 - val_loss: 4.1341 - val_accuracy: 0.1781\n",
      "Epoch 92/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.2831 - accuracy: 0.5329 - val_loss: 4.1087 - val_accuracy: 0.2055\n",
      "Epoch 93/200\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 1.4018 - accuracy: 0.4706 - val_loss: 4.2172 - val_accuracy: 0.1918\n",
      "Epoch 94/200\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.3756 - accuracy: 0.4948 - val_loss: 4.1331 - val_accuracy: 0.1918\n",
      "Epoch 95/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.5128 - accuracy: 0.4844 - val_loss: 4.3065 - val_accuracy: 0.1096\n",
      "Epoch 96/200\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.3609 - accuracy: 0.5260 - val_loss: 4.1414 - val_accuracy: 0.1781\n",
      "Epoch 97/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.3192 - accuracy: 0.5294 - val_loss: 4.1613 - val_accuracy: 0.1918\n",
      "Epoch 98/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.2840 - accuracy: 0.5536 - val_loss: 4.2872 - val_accuracy: 0.1918\n",
      "Epoch 99/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.3902 - accuracy: 0.4913 - val_loss: 4.2711 - val_accuracy: 0.1781\n",
      "Epoch 100/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.2774 - accuracy: 0.5606 - val_loss: 4.2392 - val_accuracy: 0.2055\n",
      "Epoch 101/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.2757 - accuracy: 0.5190 - val_loss: 4.3103 - val_accuracy: 0.2055\n",
      "Epoch 102/200\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.3521 - accuracy: 0.5225 - val_loss: 4.2775 - val_accuracy: 0.2055\n",
      "Epoch 103/200\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.3546 - accuracy: 0.5294 - val_loss: 4.4290 - val_accuracy: 0.1644\n",
      "Epoch 104/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.3165 - accuracy: 0.5294 - val_loss: 4.3894 - val_accuracy: 0.2055\n",
      "Epoch 105/200\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.3219 - accuracy: 0.5225 - val_loss: 4.3841 - val_accuracy: 0.2055\n",
      "Epoch 106/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.2683 - accuracy: 0.5398 - val_loss: 4.1834 - val_accuracy: 0.2192\n",
      "Epoch 107/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.2766 - accuracy: 0.5329 - val_loss: 4.0168 - val_accuracy: 0.2329\n",
      "Epoch 108/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.2010 - accuracy: 0.5675 - val_loss: 4.2596 - val_accuracy: 0.2055\n",
      "Epoch 109/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.1575 - accuracy: 0.5917 - val_loss: 4.2263 - val_accuracy: 0.2055\n",
      "Epoch 110/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1496 - accuracy: 0.5640 - val_loss: 4.3393 - val_accuracy: 0.1918\n",
      "Epoch 111/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.1530 - accuracy: 0.5433 - val_loss: 4.4355 - val_accuracy: 0.2192\n",
      "Epoch 112/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1250 - accuracy: 0.5952 - val_loss: 4.5635 - val_accuracy: 0.2055\n",
      "Epoch 113/200\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.2230 - accuracy: 0.5329 - val_loss: 4.4975 - val_accuracy: 0.2466\n",
      "Epoch 114/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1714 - accuracy: 0.5779 - val_loss: 4.4698 - val_accuracy: 0.2466\n",
      "Epoch 115/200\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.1887 - accuracy: 0.5779 - val_loss: 4.4574 - val_accuracy: 0.2466\n",
      "Epoch 116/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.2172 - accuracy: 0.5882 - val_loss: 4.4299 - val_accuracy: 0.2192\n",
      "Epoch 117/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.2075 - accuracy: 0.5882 - val_loss: 4.5280 - val_accuracy: 0.2055\n",
      "Epoch 118/200\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.1375 - accuracy: 0.5882 - val_loss: 4.6534 - val_accuracy: 0.1918\n",
      "Epoch 119/200\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.1885 - accuracy: 0.5294 - val_loss: 4.6108 - val_accuracy: 0.2192\n",
      "Epoch 120/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0484 - accuracy: 0.6540 - val_loss: 4.6195 - val_accuracy: 0.2192\n",
      "Epoch 121/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.0576 - accuracy: 0.6263 - val_loss: 4.7621 - val_accuracy: 0.2329\n",
      "Epoch 122/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.0481 - accuracy: 0.6021 - val_loss: 4.7346 - val_accuracy: 0.2329\n",
      "Epoch 123/200\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 1.0457 - accuracy: 0.6159 - val_loss: 4.7796 - val_accuracy: 0.2192\n",
      "Epoch 124/200\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.9866 - accuracy: 0.6471 - val_loss: 4.8004 - val_accuracy: 0.2329\n",
      "Epoch 125/200\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.0123 - accuracy: 0.6055 - val_loss: 4.8306 - val_accuracy: 0.2055\n",
      "Epoch 126/200\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9808 - accuracy: 0.6332 - val_loss: 4.8788 - val_accuracy: 0.2055\n",
      "Epoch 127/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.1089 - accuracy: 0.6125 - val_loss: 4.7167 - val_accuracy: 0.2055\n",
      "Epoch 128/200\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.0499 - accuracy: 0.6263 - val_loss: 4.6831 - val_accuracy: 0.2466\n",
      "Epoch 129/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.1527 - accuracy: 0.6021 - val_loss: 4.8655 - val_accuracy: 0.2329\n",
      "Epoch 130/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1317 - accuracy: 0.6436 - val_loss: 5.0782 - val_accuracy: 0.2329\n",
      "Epoch 131/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0504 - accuracy: 0.5952 - val_loss: 5.0023 - val_accuracy: 0.2466\n",
      "Epoch 132/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.0456 - accuracy: 0.6090 - val_loss: 4.9920 - val_accuracy: 0.2329\n",
      "Epoch 133/200\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.0099 - accuracy: 0.6298 - val_loss: 4.8851 - val_accuracy: 0.2192\n",
      "Epoch 134/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.0429 - accuracy: 0.5917 - val_loss: 4.9764 - val_accuracy: 0.2055\n",
      "Epoch 135/200\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.0291 - accuracy: 0.6263 - val_loss: 4.9625 - val_accuracy: 0.2055\n",
      "Epoch 136/200\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0208 - accuracy: 0.6263 - val_loss: 5.0913 - val_accuracy: 0.2055\n",
      "Epoch 137/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.9619 - accuracy: 0.6332 - val_loss: 5.0722 - val_accuracy: 0.2192\n",
      "Epoch 138/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0125 - accuracy: 0.6471 - val_loss: 5.1874 - val_accuracy: 0.1918\n",
      "Epoch 139/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0627 - accuracy: 0.6159 - val_loss: 5.1208 - val_accuracy: 0.2192\n",
      "Epoch 140/200\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.1501 - accuracy: 0.5813 - val_loss: 5.2985 - val_accuracy: 0.2055\n",
      "Epoch 141/200\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.3031 - accuracy: 0.5536 - val_loss: 5.1851 - val_accuracy: 0.1918\n",
      "Epoch 142/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.1538 - accuracy: 0.5675 - val_loss: 5.2291 - val_accuracy: 0.1781\n",
      "Epoch 143/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.2370 - accuracy: 0.5571 - val_loss: 5.2032 - val_accuracy: 0.2329\n",
      "Epoch 144/200\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 1.1342 - accuracy: 0.5848 - val_loss: 5.2459 - val_accuracy: 0.2192\n",
      "Epoch 145/200\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.0341 - accuracy: 0.6401 - val_loss: 5.1912 - val_accuracy: 0.2192\n",
      "Epoch 146/200\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1482 - accuracy: 0.5848 - val_loss: 5.2048 - val_accuracy: 0.2055\n",
      "Epoch 147/200\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 1.0438 - accuracy: 0.6332 - val_loss: 4.9188 - val_accuracy: 0.1918\n",
      "Epoch 148/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.9530 - accuracy: 0.6505 - val_loss: 5.0685 - val_accuracy: 0.2192\n",
      "Epoch 149/200\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.0925 - accuracy: 0.6125 - val_loss: 5.2652 - val_accuracy: 0.2055\n",
      "Epoch 150/200\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.0447 - accuracy: 0.6090 - val_loss: 5.2865 - val_accuracy: 0.2055\n",
      "Epoch 151/200\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.8834 - accuracy: 0.6920 - val_loss: 5.1846 - val_accuracy: 0.2192\n",
      "Epoch 152/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8657 - accuracy: 0.6713 - val_loss: 5.2691 - val_accuracy: 0.2055\n",
      "Epoch 153/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8888 - accuracy: 0.6574 - val_loss: 5.2572 - val_accuracy: 0.2055\n",
      "Epoch 154/200\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8626 - accuracy: 0.6886 - val_loss: 5.3824 - val_accuracy: 0.2055\n",
      "Epoch 155/200\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8722 - accuracy: 0.6678 - val_loss: 5.4142 - val_accuracy: 0.2055\n",
      "Epoch 156/200\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.8695 - accuracy: 0.6471 - val_loss: 5.3935 - val_accuracy: 0.2055\n",
      "Epoch 157/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0232 - accuracy: 0.6194 - val_loss: 5.4491 - val_accuracy: 0.1918\n",
      "Epoch 158/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.0299 - accuracy: 0.6194 - val_loss: 5.2612 - val_accuracy: 0.2329\n",
      "Epoch 159/200\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.5190 - accuracy: 0.5294 - val_loss: 5.3174 - val_accuracy: 0.2055\n",
      "Epoch 160/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.3606 - accuracy: 0.5398 - val_loss: 5.3909 - val_accuracy: 0.1781\n",
      "Epoch 161/200\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.1318 - accuracy: 0.6125 - val_loss: 5.2533 - val_accuracy: 0.1781\n",
      "Epoch 162/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0134 - accuracy: 0.6540 - val_loss: 5.2768 - val_accuracy: 0.2055\n",
      "Epoch 163/200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.9841 - accuracy: 0.6298 - val_loss: 5.2767 - val_accuracy: 0.2055\n",
      "Epoch 164/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0053 - accuracy: 0.6540 - val_loss: 5.2429 - val_accuracy: 0.2192\n",
      "Epoch 165/200\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9785 - accuracy: 0.6471 - val_loss: 5.2547 - val_accuracy: 0.2055\n",
      "Epoch 166/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8961 - accuracy: 0.6609 - val_loss: 5.3425 - val_accuracy: 0.2192\n",
      "Epoch 167/200\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8614 - accuracy: 0.6817 - val_loss: 5.3900 - val_accuracy: 0.2192\n",
      "Epoch 168/200\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.8187 - accuracy: 0.6920 - val_loss: 5.4736 - val_accuracy: 0.2192\n",
      "Epoch 169/200\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.8673 - accuracy: 0.6263 - val_loss: 5.5357 - val_accuracy: 0.2466\n",
      "Epoch 170/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9028 - accuracy: 0.6609 - val_loss: 5.4143 - val_accuracy: 0.2192\n",
      "Epoch 171/200\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8465 - accuracy: 0.6678 - val_loss: 5.5696 - val_accuracy: 0.2055\n",
      "Epoch 172/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.7834 - accuracy: 0.6540 - val_loss: 5.5541 - val_accuracy: 0.2329\n",
      "Epoch 173/200\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7582 - accuracy: 0.7405 - val_loss: 5.6652 - val_accuracy: 0.2055\n",
      "Epoch 174/200\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.0060 - accuracy: 0.6436 - val_loss: 5.8086 - val_accuracy: 0.2329\n",
      "Epoch 175/200\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.9702 - accuracy: 0.5952 - val_loss: 5.7612 - val_accuracy: 0.2192\n",
      "Epoch 176/200\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.8176 - accuracy: 0.7024 - val_loss: 5.8456 - val_accuracy: 0.2192\n",
      "Epoch 177/200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9172 - accuracy: 0.6747 - val_loss: 5.7944 - val_accuracy: 0.2466\n",
      "Epoch 178/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9790 - accuracy: 0.6817 - val_loss: 5.6940 - val_accuracy: 0.2329\n",
      "Epoch 179/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9854 - accuracy: 0.6298 - val_loss: 5.4937 - val_accuracy: 0.2329\n",
      "Epoch 180/200\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 0.9954 - accuracy: 0.6298 - val_loss: 5.5367 - val_accuracy: 0.2192\n",
      "Epoch 181/200\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 0.9778 - accuracy: 0.6125 - val_loss: 5.6605 - val_accuracy: 0.2192\n",
      "Epoch 182/200\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.8598 - accuracy: 0.6851 - val_loss: 5.7469 - val_accuracy: 0.2466\n",
      "Epoch 183/200\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9167 - accuracy: 0.6401 - val_loss: 5.8097 - val_accuracy: 0.2192\n",
      "Epoch 184/200\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8196 - accuracy: 0.7024 - val_loss: 5.8163 - val_accuracy: 0.2466\n",
      "Epoch 185/200\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8043 - accuracy: 0.6990 - val_loss: 5.7257 - val_accuracy: 0.2192\n",
      "Epoch 186/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9948 - accuracy: 0.6298 - val_loss: 5.5091 - val_accuracy: 0.2192\n",
      "Epoch 187/200\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9675 - accuracy: 0.6228 - val_loss: 5.5740 - val_accuracy: 0.2055\n",
      "Epoch 188/200\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.9913 - accuracy: 0.6298 - val_loss: 5.5551 - val_accuracy: 0.2055\n",
      "Epoch 189/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8834 - accuracy: 0.6851 - val_loss: 5.6432 - val_accuracy: 0.2329\n",
      "Epoch 190/200\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9080 - accuracy: 0.6782 - val_loss: 5.6872 - val_accuracy: 0.2329\n",
      "Epoch 191/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.0811 - accuracy: 0.6332 - val_loss: 5.7439 - val_accuracy: 0.2192\n",
      "Epoch 192/200\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.0549 - accuracy: 0.6332 - val_loss: 5.7099 - val_accuracy: 0.2192\n",
      "Epoch 193/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.0118 - accuracy: 0.6609 - val_loss: 5.5954 - val_accuracy: 0.2055\n",
      "Epoch 194/200\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.9196 - accuracy: 0.6920 - val_loss: 5.5641 - val_accuracy: 0.2192\n",
      "Epoch 195/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1597 - accuracy: 0.5917 - val_loss: 5.5253 - val_accuracy: 0.1918\n",
      "Epoch 196/200\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.1061 - accuracy: 0.5917 - val_loss: 5.6542 - val_accuracy: 0.2055\n",
      "Epoch 197/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9746 - accuracy: 0.6332 - val_loss: 5.5793 - val_accuracy: 0.2055\n",
      "Epoch 198/200\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9276 - accuracy: 0.6747 - val_loss: 5.5251 - val_accuracy: 0.1918\n",
      "Epoch 199/200\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9715 - accuracy: 0.6436 - val_loss: 5.3306 - val_accuracy: 0.2603\n",
      "Epoch 200/200\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.9336 - accuracy: 0.6436 - val_loss: 5.5032 - val_accuracy: 0.2329\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert action_labels to NumPy array\n",
    "action_labels = np.array(action_labels)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    state_sequences, action_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=200,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cee3bc5-d7ac-422a-9699-0c211f0ec815",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 292\n",
      "95th percentile: 223\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = [len(seq) for seq in tokenizer.texts_to_sequences(states)]\n",
    "print(f\"Max length: {max(seq_lengths)}\")\n",
    "print(f\"95th percentile: {sorted(seq_lengths)[int(len(seq_lengths) * 0.95)]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d060c38-7fcd-435e-988c-99966da86260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
